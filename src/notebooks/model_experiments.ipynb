{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Electric Vehicle ML Model Experiments\n",
        "\n",
        "This notebook contains comprehensive model training and evaluation experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure MLflow\n",
        "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
        "mlflow.set_experiment(\"EV_Model_Experiments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load feature-engineered data\n",
        "df = pd.read_csv('../data/processed/ev_data_features.csv')\n",
        "\n",
        "# Select features\n",
        "feature_cols = [\n",
        "    'Model Year', 'Electric Range', 'Base MSRP', 'vehicle_age',\n",
        "    'is_luxury', 'is_bev', 'has_long_range', 'city_ev_density',\n",
        "    'county_ev_density', 'is_urban', 'manufacturer_market_share',\n",
        "    'model_popularity', 'manufacturer_avg_range'\n",
        "]\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[feature_cols].dropna()\n",
        "y_classification = df.loc[X.index, 'is_bev']  # Classification target\n",
        "y_regression = df.loc[X.index, 'Electric Range']  # Regression target\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Classification target distribution:\\n{y_classification.value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split for classification\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "    X, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
        ")\n",
        "\n",
        "# Split for regression\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X, y_regression, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {len(X_train_clf)}\")\n",
        "print(f\"Test set size: {len(X_test_clf)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Classification Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_classifier(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Train and evaluate a classifier with MLflow tracking.\"\"\"\n",
        "    \n",
        "    with mlflow.start_run(run_name=model_name):\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "        \n",
        "        # Cross-validation score\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "        cv_mean = cv_scores.mean()\n",
        "        cv_std = cv_scores.std()\n",
        "        \n",
        "        # Log parameters\n",
        "        mlflow.log_param(\"model_type\", model_name)\n",
        "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "        \n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"accuracy\", accuracy)\n",
        "        mlflow.log_metric(\"precision\", precision)\n",
        "        mlflow.log_metric(\"recall\", recall)\n",
        "        mlflow.log_metric(\"f1_score\", f1)\n",
        "        mlflow.log_metric(\"cv_mean\", cv_mean)\n",
        "        mlflow.log_metric(\"cv_std\", cv_std)\n",
        "        \n",
        "        # Log model\n",
        "        mlflow.sklearn.log_model(model, \"model\")\n",
        "        \n",
        "        # Print results\n",
        "        print(f\"\\n{model_name} Results:\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1:.4f}\")\n",
        "        print(f\"CV Score: {cv_mean:.4f} (+/- {cv_std:.4f})\")\n",
        "        \n",
        "        return model, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multiple classifiers\n",
        "classifiers = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in classifiers.items():\n",
        "    model, accuracy = evaluate_classifier(\n",
        "        clf, X_train_clf, X_test_clf, y_train_clf, y_test_clf, name\n",
        "    )\n",
        "    results[name] = {'model': model, 'accuracy': accuracy}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "with mlflow.start_run(run_name=\"RF_GridSearch\"):\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "    grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train_clf, y_train_clf)\n",
        "    \n",
        "    # Best parameters\n",
        "    best_params = grid_search.best_params_\n",
        "    best_score = grid_search.best_score_\n",
        "    \n",
        "    # Test performance\n",
        "    y_pred = grid_search.predict(X_test_clf)\n",
        "    test_accuracy = accuracy_score(y_test_clf, y_pred)\n",
        "    \n",
        "    # Log to MLflow\n",
        "    mlflow.log_params(best_params)\n",
        "    mlflow.log_metric(\"best_cv_score\", best_score)\n",
        "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
        "    mlflow.sklearn.log_model(grid_search.best_estimator_, \"best_model\")\n",
        "    \n",
        "    print(\"Best Parameters:\")\n",
        "    for param, value in best_params.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "    print(f\"\\nBest CV Score: {best_score:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Regression Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_regressor(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Train and evaluate a regressor with MLflow tracking.\"\"\"\n",
        "    \n",
        "    with mlflow.start_run(run_name=f\"{model_name}_Regression\"):\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"mse\", mse)\n",
        "        mlflow.log_metric(\"rmse\", rmse)\n",
        "        mlflow.log_metric(\"mae\", mae)\n",
        "        mlflow.log_metric(\"r2_score\", r2)\n",
        "        \n",
        "        # Log model\n",
        "        mlflow.sklearn.log_model(model, \"model\")\n",
        "        \n",
        "        print(f\"\\n{model_name} Regression Results:\")\n",
        "        print(f\"RMSE: {rmse:.2f}\")\n",
        "        print(f\"MAE: {mae:.2f}\")\n",
        "        print(f\"R2 Score: {r2:.4f}\")\n",
        "        \n",
        "        return model, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train regression models\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "regressors = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "regression_results = {}\n",
        "for name, reg in regressors.items():\n",
        "    model, r2 = evaluate_regressor(\n",
        "        reg, X_train_reg, X_test_reg, y_train_reg, y_test_reg, name\n",
        "    )\n",
        "    regression_results[name] = {'model': model, 'r2': r2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance from best model\n",
        "best_model = results['Random Forest']['model']\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': best_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare classification models\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Classification comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "clf_scores = [results[name]['accuracy'] for name in results]\n",
        "plt.bar(results.keys(), clf_scores, color='skyblue')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Classification Model Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim([0.8, 1.0])\n",
        "\n",
        "# Regression comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "reg_scores = [regression_results[name]['r2'] for name in regression_results]\n",
        "plt.bar(regression_results.keys(), reg_scores, color='lightcoral')\n",
        "plt.ylabel('R2 Score')\n",
        "plt.title('Regression Model Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylim([0.5, 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Best Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save best classification model\n",
        "best_clf = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
        "joblib.dump(best_clf[1]['model'], '../models/saved/best_classifier.pkl')\n",
        "print(f\"Best classifier saved: {best_clf[0]} with accuracy {best_clf[1]['accuracy']:.4f}\")\n",
        "\n",
        "# Save best regression model\n",
        "best_reg = max(regression_results.items(), key=lambda x: x[1]['r2'])\n",
        "joblib.dump(best_reg[1]['model'], '../models/saved/best_regressor.pkl')\n",
        "print(f\"Best regressor saved: {best_reg[0]} with R2 score {best_reg[1]['r2']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Experiment Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": "null",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nClassification Models:\")\n",
        "print(\"-\"*30)\n",
        "for name, result in results.items():\n",
        "    print(f\"{name:20} Accuracy: {result['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nRegression Models:\")\n",
        "print(\"-\"*30)\n",
        "for name, result in regression_results.items():\n",
        "    print(f\"{name:20} R2 Score: {result['r2']:.4f}\")\n",
        "\n",
        "print(\"\\nBest Models:\")\n",
        "print(\"-\"*30)\n",
        "print(f\"Classification: {best_clf[0]}\")\n",
        "print(f\"Regression: {best_reg[0]}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}